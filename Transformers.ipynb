{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beb34f8e-e013-48ee-9ec4-3519ba051c2f",
   "metadata": {},
   "source": [
    "**Load Tokenized Dataset or Download the base dataset (not tokenized) from hf**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b336235-c97b-4b7e-80e5-16d66d475ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "from pathlib import Path\n",
    "\n",
    "#Initialize a bool to check if the tokenized dataset exists in disk or if it has to be tokenized \n",
    "tokenized_dataset_exists = False\n",
    "\n",
    "dataset_path = Path(\"./data/tokenized_dataset\")\n",
    "\n",
    "if dataset_path.exists():\n",
    "    tokenized_dataset = load_from_disk(str(dataset_path))\n",
    "    tokenized_dataset_exists = True\n",
    "    print(\"Dataset loaded successfully!\")\n",
    "else:\n",
    "    print(\"Dataset does not exist at the specified path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b858135-492c-4432-b8c9-75cd5352c538",
   "metadata": {},
   "source": [
    "**If the tokenized dataset doesnt exist on disk we download it and tokenize it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39981374-6d95-45a1-9cf2-7c2a9331f58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping the download of whole dataset from Huggingface\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "if tokenized_dataset_exists == False:\n",
    "    # Load the dataset and specify the cache directory\n",
    "    whole_dataset = load_dataset(\"wikimedia/wikipedia\", \"20231101.es\", cache_dir=\"./data\")\n",
    "    ds = ds[\"train\"].train_test_split(test_size=0.98, seed=42) #We split the dataset to make it smaller and \n",
    "    #try different training parameters faster\n",
    "else:\n",
    "    print(\"Skipping the download of whole dataset from Huggingface\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e9a7a9-3660-416f-8f70-eee2b321f5a9",
   "metadata": {},
   "source": [
    "## We load the tokenizer from disk or create the tokenizer if it doesnt exist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cb96ad-db75-4c61-a223-9269027d7c8e",
   "metadata": {},
   "source": [
    "Function to load the text from the dataset to tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a69b9a2-6cb9-4456-bd38-03e32befdaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_corpus():\n",
    "    for sample in ds[\"train\"]:\n",
    "        yield sample[\"text\"]  # Extract text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c31694-6d8c-4fbc-9ebd-128c0d1cb93c",
   "metadata": {},
   "source": [
    "**Load tokenizer // Create tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f582584-3b7a-4bfb-aacb-b9f921e9f5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded! üéâ\n",
      "[42, 8735, 21776, 3]\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from datasets import load_from_disk\n",
    "from pathlib import Path\n",
    "\n",
    "#Initialize a bool to check if the tokenized dataset exists in disk or if it has to be tokenized \n",
    "tokenized_dataset_exists = False\n",
    "\n",
    "tokenizer_path = Path(\"./data/scratch_tokenizer.json\")\n",
    "\n",
    "if tokenizer_path.exists():\n",
    "    #load tokenizer if its saved on disk\n",
    "    hf_tokenizer = PreTrainedTokenizerFast(tokenizer_file=str(tokenizer_path), \n",
    "                                       unk_token=\"[UNK]\", \n",
    "                                       pad_token=\"[PAD]\", \n",
    "                                       mask_token=\"[MASK]\")\n",
    "    # Save it in the Hugging Face format\n",
    "    hf_tokenizer.save_pretrained(\"custom_tokenizer\")\n",
    "    print(\"Tokenizer loaded! üéâ\")\n",
    "else:\n",
    "    tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "    trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[MASK]\"], vocab_size=30_000)\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)\n",
    "    tokenizer.save(\"./data/scratch_tokenizer.json\")\n",
    "    print(\"Tokenizer training complete! üéâ\")\n",
    "\n",
    "\n",
    "# Test with Transformers API\n",
    "print(hf_tokenizer.encode(\"Hello world!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5879d5-413e-455a-b561-b8787f731641",
   "metadata": {},
   "source": [
    "**Tokenize the dataset if not in disk**\n",
    "\n",
    "At the beginning we downloaded the whole dataset in the case the tokenized one didnt exist\n",
    "but we havent tokenized it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13d39b0f-15ec-44d3-bfba-d7d2be6ae4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize text in 128-token chunks\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize text and split into chunks of max length 128\n",
    "    tokenized_text = hf_tokenizer(\n",
    "        examples[\"text\"], \n",
    "        truncation=True, \n",
    "        padding=\"max_length\", \n",
    "        max_length=128\n",
    "    )\n",
    "    return tokenized_text\n",
    "    \n",
    "if not dataset_path.exists():\n",
    "    # Apply tokenization to dataset\n",
    "    tokenized_dataset = ds[\"train\"].map(tokenize_function, batched=True)\n",
    "    tokenized_dataset.save_to_disk(str(dataset_path))\n",
    "    print(f\"Dataset tokenized and saved to disk on path {string(dataset_path)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad72fbf2-287d-44f5-bc6f-d94457520093",
   "metadata": {},
   "source": [
    "## Creating a data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7c95a2f-2f97-4423-9ea5-b07698665dc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([36823, 128])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Convert dataset to PyTorch format\n",
    "tokenized_text = torch.tensor(tokenized_dataset[\"input_ids\"])\n",
    "tokenized_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7aba66b-803f-43b9-9881-01382baa2651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Data Shape: torch.Size([36823, 127])\n",
      "Target Data Shape: torch.Size([36823, 127])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Shift left to create targets\n",
    "input_data = tokenized_text[:, :-1]  # Remove LAST token in each sequence\n",
    "target_data = tokenized_text[:, 1:]  # Remove FIRST token in each sequence\n",
    "\n",
    "print(\"Input Data Shape:\", input_data.shape)  # Should be (6455, 127)\n",
    "print(\"Target Data Shape:\", target_data.shape)  # Should be (6455, 127)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ab4897d-c94b-46c8-8872-c1239b24d0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "batch_size = 64\n",
    "dataset = TensorDataset(input_data, target_data)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True,  num_workers=4, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bae10942-c80d-47cd-875e-a400e3c166d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Both pytorch and HF tokenizer will want to use many cpu cores, and will result in error\n",
    "## Since we already tokenized the whole corpus we can disable the parallelism of the hf tokenizer\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82b66cd9-d30c-4c9a-8dc0-f6e0dd6704e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch input shape: torch.Size([64, 127])\n",
      "Batch target shape: torch.Size([64, 127])\n"
     ]
    }
   ],
   "source": [
    "# Check batch shapes\n",
    "for batch in data_loader:\n",
    "    inputs, targets = batch\n",
    "    print(\"Batch input shape:\", inputs.shape)\n",
    "    print(\"Batch target shape:\", targets.shape)\n",
    "    break  # St"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e22e1eb-2661-4b8a-ac23-50d2ecca23fe",
   "metadata": {},
   "source": [
    "## Attention Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2ca3a4-9e43-46f7-984a-c4d40a8f3914",
   "metadata": {},
   "source": [
    "**Positional encodings**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3c6aa2e-df0e-4657-a7d0-927df9ac5d36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  4,  6],\n",
       "        [ 8, 10, 12]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.tensor([[1,2,3],[4,5,6]])\n",
    "b = torch.tensor( [2, 2, 2])\n",
    "c = a* b\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf24d096-211f-4fe5-9ec8-bb865837b7ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4aff2943-af5f-42f0-a968-0b60efa35251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 127, 200])\n",
      "Positional Encoding Shape: torch.Size([4, 127, 200])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "def get_positional_encodings(max_seq_length, d_model, device):\n",
    "    p_e = torch.zeros(max_seq_length, d_model, device = device)  # Shape (L, D)\n",
    "    position = torch.arange(0, max_seq_length, dtype=torch.float32).unsqueeze(1)  # Shape (L, 1)\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float32) * (-math.log(10000.0) / d_model))\n",
    "\n",
    "    p_e[:, 0::2] = torch.sin(position * div_term)\n",
    "    p_e[:, 1::2] = torch.cos(position * div_term)\n",
    "    \n",
    "    return p_e  # Shape (L, D)\n",
    "\n",
    "# Test Code\n",
    "batch_size = 4\n",
    "seq_length = 127  # This corresponds to max_seq_length in the function\n",
    "d_model = 200\n",
    "\n",
    "\n",
    "# Generate positional encodings for the given sequence length and model dimension\n",
    "pos_enc = get_positional_encodings(seq_length, d_model, device)  # Shape (L, D)\n",
    "print(pos_enc.unsqueeze(0).shape)\n",
    "# Expanding for batch size\n",
    "pos_enc_batch = pos_enc.unsqueeze(0).expand(batch_size, -1, -1)  # Shape (B, L, D)\n",
    "\n",
    "# Print the shape to verify\n",
    "print(\"Positional Encoding Shape:\", pos_enc_batch.shape)  # Expected: (4, 10, 16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ea4e6d-9631-4b1a-a09d-64b4ebe3095c",
   "metadata": {},
   "source": [
    "**Attention Head**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52678667-5a09-4bb0-b6f7-5d5428ef5f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention output shape: torch.Size([4, 10, 64])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class SelfAttentionHead(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        \"\"\"\n",
    "        Initializes a self-attention head.\n",
    "        \n",
    "        Args:\n",
    "            embedding_dim (int): The dimension of the input embeddings (d_model).\n",
    "        \"\"\"\n",
    "        super(SelfAttentionHead, self).__init__()\n",
    "        # Define the linear projection layers for queries, keys, and values\n",
    "        self.q_layer = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.k_layer = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.v_layer = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.layer_norm = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        ## FFN - Feed Forward Network\n",
    "        self.first_linear = nn.Linear(embedding_dim, 4 * embedding_dim) ## Amplifies the att output \n",
    "        \n",
    "        self.relu_layer = nn.ReLU() #Relu between them\n",
    "\n",
    "        self.second_linear = nn.Linear( 4 * embedding_dim, embedding_dim) ## Reduces the att output back to embedding size\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Perform the self-attention operation.\n",
    "        \n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (batch_size, seq_length, embedding_dim)\n",
    "        \n",
    "        Returns:\n",
    "            att_value (Tensor): The attention output of shape (batch_size, seq_length, embedding_dim)\n",
    "        \"\"\"\n",
    "        # Compute queries, keys, and values\n",
    "        Q = self.q_layer(x)  # (batch_size, seq_length, embedding_dim)\n",
    "        K = self.k_layer(x)  # (batch_size, seq_length, embedding_dim)\n",
    "        V = self.v_layer(x)  # (batch_size, seq_length, embedding_dim)\n",
    "        \n",
    "        # Compute the dot-product attention scores and scale them\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1))  # (batch_size, seq_length, seq_length)\n",
    "        scores = scores / math.sqrt(Q.size(-1))\n",
    "        mask = torch.triu(torch.ones(x.size(1), x.size(1), device=x.device), diagonal=1).bool()\n",
    "\n",
    "        # Expand mask dimensions for batch (or rely on broadcasting)\n",
    "        scores = scores.masked_fill(mask, float('-inf'))\n",
    "        \n",
    "        # Apply softmax to obtain attention weights\n",
    "        att_weights = F.softmax(scores, dim=-1)  # (batch_size, seq_length, seq_length)\n",
    "        \n",
    "        # Multiply attention weights by the values to get the final output\n",
    "        att_value = torch.matmul(att_weights, V)  # (batch_size, seq_length, embedding_dim)\n",
    "\n",
    "        #Normalization\n",
    "        normalized_attention_values = self.layer_norm(att_value)\n",
    "\n",
    "        #FFW\n",
    "        amplified_attention = self.first_linear(normalized_attention_values) #First forward layer\n",
    "        \n",
    "        amplified_attention = self.relu_layer(amplified_attention) #Relu\n",
    "        \n",
    "        attention_logits = self.second_linear(amplified_attention) #Second forward layer\n",
    "        return attention_logits\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Hyperparameters\n",
    "    batch_size = 4\n",
    "    seq_length = 10\n",
    "    embedding_dim = 64\n",
    "\n",
    "    # Dummy input tensor\n",
    "    x_test = torch.randn(batch_size, seq_length, embedding_dim)\n",
    "    \n",
    "    # Create the self-attention head instance\n",
    "    attention_head = SelfAttentionHead(embedding_dim)\n",
    "    \n",
    "    # Run the forward pass\n",
    "    output = attention_head(x_test)\n",
    "    print(\"Attention output shape:\", output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb46cec0-6947-4dab-8211-c9ea38135092",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Transformers(nn.Module):\n",
    "    def __init__(self, max_seq_length, vocab_size, embedding_dim):\n",
    "        super(Transformers, self).__init__()\n",
    "        \n",
    "        self.pos_enc = get_positional_encodings(max_seq_length-1, d_model, device).unsqueeze(0)  # Shape (1, L, D)\n",
    "\n",
    "        # üîπ First layer: Embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.attention_head = SelfAttentionHead(embedding_dim)\n",
    "\n",
    "        self.logits = nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, sequence_length]\n",
    "        \n",
    "        embedded = self.embedding(x)  # üé≠ Convert indices to embeddings\n",
    "        pos_enc = self.pos_enc # pos_enc is (1, L, embedding_dim)\n",
    "        pos_enc = pos_enc[:, :embedded.size(1), :]  # slice to the input length\n",
    "        x = embedded + pos_enc\n",
    "\n",
    "        attention_value = self.attention_head(x)\n",
    "\n",
    "        logits = self.logits(attention_value)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f8027f-5942-4d5f-9c79-09c7796ced83",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6635eb69-ee81-456b-a4c6-3ace2c241f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim \n",
    "import torch.amp as amp  # For Automatic Mixed Precision (AMP)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "vocab_size = 30000   # Number of words in vocabulary\n",
    "\n",
    "#d_model\n",
    "embedding_dim = 200  # from 100 to 200, for instance\n",
    "\n",
    "max_seq_lenght = 128\n",
    "\n",
    "model = Transformers(max_seq_lenght, vocab_size, embedding_dim)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr =0.003\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "num_epochs = 10\n",
    "scaler = amp.GradScaler()  # GradScaler for AMP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9041f6-a74a-4937-ac12-a1aadf72b1a2",
   "metadata": {},
   "source": [
    "**Testing the transformer model witn an input of shape (B, L, D)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47ec39e2-6abf-474b-b58d-15ccdb47db17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 127])\n",
      "Expected Shape: (32, 127, 30000)\n",
      "Actual Shape: torch.Size([32, 127, 30000])\n",
      "‚úÖ Transformer test passed!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate random input of shape (B, L, D)\n",
    "input_tensor = torch.randint(0, vocab_size, (batch_size, max_seq_lenght-1)).long().to(device)\n",
    "# Forward pass\n",
    "output = model(input_tensor)\n",
    "\n",
    "# Assertions to validate correct output\n",
    "print(\"Input shape:\", input_tensor.shape)  # Expected: (B, L, D)\n",
    "print(f\"Expected Shape: ({batch_size}, {max_seq_lenght-1}, {vocab_size})\")\n",
    "print(f\"Actual Shape: {output.shape}\")\n",
    "\n",
    "assert output.shape == (batch_size, max_seq_lenght-1, vocab_size), \"‚ùå Mismatch in output shape!\"\n",
    "print(\"‚úÖ Transformer test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8662a886-ccb1-4bee-b3c1-e01d078c4619",
   "metadata": {},
   "source": [
    "## Use dataloader batches for smaller inputs for memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75c61af6-53b2-4448-b594-aaf55b5adb76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68f1651-5312-48d2-8658-9dd42895cc0b",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7ded7cce-9ee3-4062-aec0-654b8f3c8807",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Summary:\n",
      "  Total Epoch Time     : 37.98 s\n",
      "  Total Data Loading   : 0.14 s\n",
      "  Total GPU Compute    : 34.74 s\n",
      "  Average Loss         : 3.6463\n",
      "\n",
      "Epoch 2 Summary:\n",
      "  Total Epoch Time     : 38.76 s\n",
      "  Total Data Loading   : 0.14 s\n",
      "  Total GPU Compute    : 35.43 s\n",
      "  Average Loss         : 3.5808\n",
      "\n",
      "Epoch 3 Summary:\n",
      "  Total Epoch Time     : 38.03 s\n",
      "  Total Data Loading   : 0.14 s\n",
      "  Total GPU Compute    : 34.78 s\n",
      "  Average Loss         : 3.5238\n",
      "\n",
      "Epoch 4 Summary:\n",
      "  Total Epoch Time     : 37.11 s\n",
      "  Total Data Loading   : 0.13 s\n",
      "  Total GPU Compute    : 33.94 s\n",
      "  Average Loss         : 3.4707\n",
      "\n",
      "Epoch 5 Summary:\n",
      "  Total Epoch Time     : 37.16 s\n",
      "  Total Data Loading   : 0.13 s\n",
      "  Total GPU Compute    : 33.98 s\n",
      "  Average Loss         : 3.4239\n",
      "\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start = time.time()\n",
    "    total_data_loading_time = 0.0\n",
    "    total_gpu_compute_time = 0.0\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # Initialize batch_start to measure data loading for the first batch.\n",
    "    batch_start = time.time()\n",
    "\n",
    "    for batch in data_loader:\n",
    "        # Once batch is fetched, measure data loading time:\n",
    "        batch_loaded_time = time.time()\n",
    "        total_data_loading_time += (batch_loaded_time - batch_start)\n",
    "        \n",
    "        # Unpack and send to device:\n",
    "        batch_inputs, batch_targets = batch\n",
    "        batch_inputs = batch_inputs.to(device).long()\n",
    "        batch_targets = batch_targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Start GPU compute timing:\n",
    "        gpu_start = time.time()\n",
    "        with amp.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "            outputs = model(batch_inputs)\n",
    "            loss = criterion(outputs.view(-1, vocab_size), batch_targets.view(-1))\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        gpu_end = time.time()\n",
    "        total_gpu_compute_time += (gpu_end - gpu_start)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Prepare for next iteration: record time after optimizer step.\n",
    "        batch_start = time.time()\n",
    "\n",
    "    epoch_end = time.time()\n",
    "    epoch_duration = epoch_end - epoch_start\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} Summary:\")\n",
    "    print(f\"  Total Epoch Time     : {epoch_duration:.2f} s\")\n",
    "    print(f\"  Total Data Loading   : {total_data_loading_time:.2f} s\")\n",
    "    print(f\"  Total GPU Compute    : {total_gpu_compute_time:.2f} s\")\n",
    "    print(f\"  Average Loss         : {avg_loss:.4f}\\n\")\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b03323b6-440d-497a-8e2a-ef19e6527d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def generate_text(model, tokenizer, prompt, num_words=50, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Generate text using the  model.\n",
    "\n",
    "    Args:\n",
    "    - model: \n",
    "    - tokenizer: Tokenizer with `encode()` and `decode()` methods.\n",
    "    - prompt: Seed text to start generation.\n",
    "    - num_words: Number of words to generate.\n",
    "    - device: \"cuda\" or \"cpu\".\n",
    "\n",
    "    Returns:\n",
    "    - Generated text (string).\n",
    "    \"\"\"\n",
    "    model.eval()  # ‚úÖ Set model to evaluation mode\n",
    "    model.to(device)\n",
    "\n",
    "    # üëá Tokenize the input prompt\n",
    "    input_ids = torch.tensor(tokenizer.encode(prompt), dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    for _ in range(num_words):\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids)  # üîç Forward pass\n",
    "            next_token_logits = logits[:, -1, :]  # Take last token's output\n",
    "\n",
    "            # üé≤ Sample the next word (greedy or probabilistic)\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(0)  # Greedy decoding\n",
    "\n",
    "            # Append to input sequence\n",
    "            input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "\n",
    "    # üîÑ Convert token IDs back to text\n",
    "    generated_text = tokenizer.decode(input_ids.squeeze().tolist())\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1000217e-d3aa-4e3b-b9ce-015d6c6c0f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Argentina es un pais muy maravillos o , que se encuentra en la ciudad de San Juan , en la provincia de Buenos Aires , Argentina . Fue uno de los m√°s importantes de la ciudad de Trujillo\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate text\n",
    "prompt_text = \"Argentina es un pais muy maravilloso,\"\n",
    "generated_story = generate_text(model, hf_tokenizer, prompt_text, num_words=30)\n",
    "print(generated_story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e5905f-f074-49bd-8d24-fb8cbc7d2dc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
